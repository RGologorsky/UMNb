{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4bbb5eb",
   "metadata": {},
   "source": [
    "# Adapted from\n",
    "\n",
    "https://github.com/Project-MONAI/tutorials/blob/main/modules/transfer_mmar.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ee6470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install monai\n",
    "# !python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, lmdb, tqdm]\"\n",
    "\n",
    "import os, sys, shutil, time, pickle, glob\n",
    "from pathlib import Path\n",
    "\n",
    "# numpy to SITK conversion\n",
    "import torch\n",
    "import numpy     as np\n",
    "import SimpleITK as sitk\n",
    "\n",
    "# hardware stats\n",
    "import GPUtil as GPU\n",
    "\n",
    "# plot\n",
    "from helpers.viz import viz_axis, viz_compare_inputs, viz_compare_outputs\n",
    "from helpers.viz import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MONAI\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import DiceFocalLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import (\n",
    "    Dataset,\n",
    "    CacheDataset,\n",
    "    LMDBDataset,\n",
    "    DataLoader,\n",
    "    decollate_batch,\n",
    ")\n",
    "\n",
    "from monai.networks.utils import copy_model_state\n",
    "from monai.optimizers import generate_param_groups\n",
    "\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AddChanneld,\n",
    "    CenterSpatialCropd,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    NormalizeIntensityd,\n",
    "    PadListDataCollate,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    SpatialPadd,\n",
    "    Orientationd,\n",
    "    CropForegroundd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandAffined,\n",
    "    RandRotated,\n",
    "    EnsureType,\n",
    "    EnsureTyped,\n",
    "    ToTensord,\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111b7fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONI:  /home/gologors/data/pitmri/las_pt/loni_im_ABIDE_50136_MRI_MP-RAGE_br_raw_20120830202014457_S165053_I329063.pt\n",
      "PIT:  /home/gologors/data/pitmri/las_pt/pit_im_1.3.46.670589.11.37169.5.0.8820.2016110101521229038_COR_T1_CLEAR_20161101012852_701.pt\n",
      "total number of images: 387\n",
      "number of images for training: 382\n",
      "number of images for val: 5\n",
      "number of images for testing: 0\n",
      "total number of images: 387\n",
      "number of images for training: 382\n",
      "number of images for val: 5\n",
      "number of images for testing: 0\n"
     ]
    }
   ],
   "source": [
    "# Get labels\n",
    "\n",
    "root = \"/home/gologors/data/\"\n",
    "\n",
    "with open(root + 'pitmri/' + 'all_filenames_pt.pkl', 'rb') as f:\n",
    "    all_filenames_pt = pickle.load(f)\n",
    "\n",
    "with open(root + 'pitmri/' + 'loni_filenames.pkl', 'rb') as f: \n",
    "    loni_filenames = pickle.load(f)\n",
    "    \n",
    "# with open(root + 'pitmri/' + 'all_filenames_nii.pkl', 'rb') as f: \n",
    "#     all_filenames = pickle.load(f)\n",
    "    \n",
    "with open(root + 'pitmri/' + 'las_nii_im_datadict_pt.pkl', 'rb') as f:\n",
    "     im_datadict_pt = pickle.load(f)\n",
    "\n",
    "with open(root + 'pitmri/' + 'loni_bboxs_pt.pkl', 'rb') as f:\n",
    "    loni_bboxs = pickle.load(f)\n",
    "    \n",
    "loni_filenames = all_filenames_pt[:len(loni_filenames)]\n",
    "pit_filenames  = all_filenames_pt[len(loni_filenames):]\n",
    "\n",
    "print(\"LONI: \", loni_filenames[0][0])\n",
    "print(\"PIT: \", pit_filenames[0][0])\n",
    "\n",
    "# Split into training/valid and testing \n",
    "# adapted from https://github.com/Project-MONAI/tutorials/blob/main/modules/autoencoder_mednist.ipynb\n",
    "\n",
    "# Split pit filenames\n",
    "\n",
    "def split(lst, test_frac, valid_frac):\n",
    "\n",
    "    num_test  = int(len(lst) * test_frac)\n",
    "    num_valid = int(len(lst) * valid_frac)\n",
    "    num_train = len(lst) - num_test - num_valid\n",
    "\n",
    "    train_datadict = [{\"im\": nii, \"lbl\":obj} for nii,obj in lst[0:num_train]]\n",
    "    valid_datadict = [{\"im\": nii, \"lbl\":obj} for nii,obj in lst[num_train:num_train+num_valid]]\n",
    "    test_datadict = [{\"im\": nii, \"lbl\": obj} for nii,obj in lst[-num_test:]]\n",
    "    \n",
    "    return train_datadict, valid_datadict, test_datadict\n",
    "\n",
    "\n",
    "pit_train_datadict, pit_valid_datadict, pit_test_datadict    = split(pit_filenames, test_frac=0, valid_frac=0.1) # 50: 45-5-0\n",
    "loni_train_datadict, loni_valid_datadict, loni_test_datadict = split(loni_filenames, test_frac=0, valid_frac=0.) # 337 all train\n",
    "\n",
    "# for loni, add info on start/end\n",
    "def add_coronal_info(lst):\n",
    "    new = []\n",
    "    for i,d in enumerate(lst):\n",
    "        item    = (d[\"im\"],d[\"lbl\"])\n",
    "        is_loni = (\"loni\" in d[\"im\"])\n",
    "        \n",
    "        # add coronal slices as seperate items\n",
    "        if is_loni:\n",
    "            loni_bbox = loni_bboxs[loni_filenames.index(item)][\"bbox\"]\n",
    "            start = loni_bbox[2]\n",
    "            end   = loni_bbox[3]\n",
    "        else:\n",
    "            shape = im_datadict_pt[all_filenames_pt.index(item)][\"shape\"]        \n",
    "            start = 0\n",
    "            end   = shape[1]\n",
    "        d1 = {k:v for k,v in d.items()}\n",
    "        d1[\"start\"] = start\n",
    "        d1[\"end\"] = end\n",
    "        new.append(d1)\n",
    "    return new\n",
    "    \n",
    "train_datadict = pit_train_datadict + loni_train_datadict\n",
    "valid_datadict = pit_valid_datadict\n",
    "test_datadict = {}\n",
    "\n",
    "print(f\"total number of images: {len(pit_filenames) + len(loni_filenames)}\")\n",
    "print(f\"number of images for training: {len(train_datadict)}\")\n",
    "print(f\"number of images for val: {len(valid_datadict)}\")\n",
    "print(f\"number of images for testing: {len(test_datadict)}\")\n",
    "\n",
    "# Expand to 2d\n",
    "def expand_2d(lst):\n",
    "    twod = []\n",
    "    \n",
    "    for d in lst:\n",
    "        item    = (d[\"im\"],d[\"lbl\"])\n",
    "        is_loni = (\"loni\" in d[\"im\"])\n",
    "        \n",
    "        # add coronal slices as seperate items\n",
    "        if is_loni:\n",
    "            loni_bbox = loni_bboxs[loni_filenames.index(item)][\"bbox\"]\n",
    "            start = loni_bbox[2]\n",
    "            end   = loni_bbox[3]\n",
    "        else:\n",
    "            shape = im_datadict_pt[all_filenames_pt.index(item)][\"shape\"]        \n",
    "            start = 0\n",
    "            end   = shape[1]\n",
    "            \n",
    "        for i in range(start, end):\n",
    "            twod.append({\"im\": d[\"im\"], \"lbl\": d[\"lbl\"], \"i\": i})\n",
    "    return twod\n",
    "            \n",
    "# train_datadict = expand_2d(train_datadict)\n",
    "# valid_datadict = expand_2d(valid_datadict)\n",
    "# test_datadict = expand_2d(test_datadict)\n",
    "\n",
    "train_datadict = add_coronal_info(train_datadict)\n",
    "valid_datadict = add_coronal_info(valid_datadict)\n",
    "test_datadict = add_coronal_info(test_datadict)\n",
    "\n",
    "print(f\"total number of images: {len(pit_filenames) + len(loni_filenames)}\")\n",
    "print(f\"number of images for training: {len(train_datadict)}\")\n",
    "print(f\"number of images for val: {len(valid_datadict)}\")\n",
    "print(f\"number of images for testing: {len(test_datadict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40daca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for d in train_datadict + valid_datadict + test_datadict:\n",
    "    print(d[\"end\"] - d[\"start\"])\n",
    "    l.append(d[\"end\"] - d[\"start\"])\n",
    "\n",
    "\n",
    "def mask2bbox_pt(mask):\n",
    "    \"\"\"Returns bounding box coordinates of binary mask\"\"\"\n",
    "    k = torch.any(torch.any(mask, dim=0), dim=0) # 0 -> 1,2 -> 1 -> 2 left\n",
    "    j = torch.any(torch.any(mask, dim=0), dim=1) # 0 -> 1,2 -> 2 -> 1 left\n",
    "    i = torch.any(torch.any(mask, dim=1), dim=1) # 1 -> 0,2 -> 0 -> 0 left\n",
    "    \n",
    "    imin, imax = torch.where(i)[0][[0, -1]]\n",
    "    jmin, jmax = torch.where(j)[0][[0, -1]]\n",
    "    kmin, kmax = torch.where(k)[0][[0, -1]]\n",
    "    \n",
    "    # inclusive indices\n",
    "    return torch.tensor([imin, imax+1, jmin, jmax+1, kmin, kmax+1])\n",
    "\n",
    "def sitk2np(obj): return np.swapaxes(sitk.GetArrayFromImage(obj), 0, 2)\n",
    "def np2sitk(arr): return sitk.GetImageFromArray(np.swapaxes(arr, 0, 2))\n",
    "\n",
    "def torch2sitk(t): return sitk.GetImageFromArray(torch.transpose(t, 0, 2))\n",
    "def sitk2torch(o): return torch.transpose(torch.tensor(sitk.GetArrayFromImage(o)), 0, 2)\n",
    "\n",
    "\n",
    "def load_pt_6ch(x):\n",
    "    d = {}\n",
    "    im_fn, lbl_fn, start, end = x[\"im\"], x[\"lbl\"], x[\"start\"], x[\"end\"]\n",
    "    \n",
    "    # index first 2 slices, mid 2 slices, last 2 slices. min is 7\n",
    "    mid = torch.div(start+end, 2, rounding_mode='trunc') # (start+end)//2\n",
    "    idx1 = torch.tensor([start, start+1, mid, mid+1, end-2, end-1])\n",
    "\n",
    "    # bring 6 channels to first dim eg 6x180x180\n",
    "    d[\"im\"]  = torch.load(im_fn)[:,idx1,:].squeeze().transpose(0,1)\n",
    "    d[\"lbl\"] = torch.load(lbl_fn)[:,idx1,:].squeeze().transpose(0,1)\n",
    "\n",
    "    return d\n",
    "\n",
    "def load_pt_1ch(x):\n",
    "    d = {}\n",
    "    im_fn, lbl_fn, start, end = x[\"im\"], x[\"lbl\"], x[\"start\"], x[\"end\"]\n",
    "    \n",
    "    # index first 2 slices, mid 2 slices, last 2 slices. min is 7\n",
    "    #mid = (start+end)//2\n",
    "    mid = torch.div(start+end, 2, rounding_mode='trunc') # (start+end)//2\n",
    "    #idx1 = torch.tensor([start, start+1, mid, mid+1, end-2, end-1])\n",
    "\n",
    "    # bring 6 channels to first dim eg 6x180x180\n",
    "    d[\"im\"]  = torch.load(im_fn)[:,mid,:].squeeze()\n",
    "    d[\"lbl\"] = torch.load(lbl_fn)[:,mid,:].squeeze()\n",
    "\n",
    "    return d\n",
    "\n",
    "# loni_bboxs = []\n",
    "# for im_fn, lbl_fn in loni_filenames:\n",
    "#     lbl = sitk.ReadImage(lbl_fn, sitk.sitkUInt8)\n",
    "#     bbox = mask2bbox_pt(sitk2torch(lbl))\n",
    "#     loni_bboxs.append({\"im\": im_fn, \"lbl\": lbl_fn, \"bbox\": bbox})\n",
    "\n",
    "def get_stem_name_pt(fn):\n",
    "    basename = os.path.basename(fn)\n",
    "    return basename[:basename.index(\".pt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "\n",
    "largest_sz_pit     = (576, 42, 640)\n",
    "largest_sz_loni    = (256, 68, 512)\n",
    "\n",
    "largest_sz         = (640, 640)\n",
    "center_crop_sz     = (288, 288)\n",
    "\n",
    "train_datadict[1]\n",
    "\n",
    "# Transforms\n",
    "from helpers.transforms_simplified import UndoDict\n",
    "\n",
    "keys=[\"im\", \"lbl\"]\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        load_pt_1ch,\n",
    "        NormalizeIntensityd(keys=[\"im\"], nonzero=True, channel_wise=False),\n",
    "        AddChanneld(keys=keys),\n",
    "        SpatialPadd(keys=keys, spatial_size=largest_sz, method=\"symmetric\", mode=\"constant\"),\n",
    "        CenterSpatialCropd(keys=keys, roi_size=center_crop_sz),\n",
    "        UndoDict(keys=[\"im\", \"lbl\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "valid_transforms = train_transforms\n",
    "\n",
    "check_ds = Dataset(data=valid_datadict, transform=valid_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=2)\n",
    "\n",
    "count_ims = 0\n",
    "for check_data in check_loader:\n",
    "    print(check_data[0].shape)\n",
    "    image, label = check_data[0][1][0], check_data[1][1][0] #(check_data[\"im\"][0][0], check_data[\"lbl\"][0][0])\n",
    "    print(f\"image shape: {image.shape}, label shape: {label.shape}\")\n",
    "    #plot the slice [:, :, 21]\n",
    "    plt.figure(\"check\", (12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"image\")\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"label\")\n",
    "    plt.imshow(label)\n",
    "    plt.show()\n",
    "    \n",
    "    count_ims += 1\n",
    "    \n",
    "    if count_ims == 3:\n",
    "        break\n",
    "\n",
    "train_datadict[0]\n",
    "\n",
    "load_pt_6ch(train_datadict[1])[\"lbl\"].shape\n",
    "\n",
    "idx1 = torch.tensor([ 0,  1,  8,  9, 14, 15])\n",
    "\n",
    "im = torch.load(train_datadict[9][\"im\"])\n",
    "im2 = im[:,idx1,:].squeeze()\n",
    "\n",
    "im2.shape\n",
    "\n",
    "count_ims = 0\n",
    "\n",
    "fns = [valid_datadict[0], train_datadict[0], train_datadict[100]]\n",
    "\n",
    "for fn in fns:\n",
    "    data = valid_transforms(fn)\n",
    "    print(data[0][0].shape)\n",
    "    image, label = data[0][0], data[1][0] #(check_data[\"im\"][0][0], check_data[\"lbl\"][0][0])\n",
    "    print(os.path.basename(fn[\"im\"]))\n",
    "    \n",
    "    print(f\"image shape: {image.shape}, label shape: {label.shape}\")\n",
    "    #plot the slice [:, :, 21]\n",
    "    plt.figure(\"check\", (12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"image\")\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"label\")\n",
    "    plt.imshow(label)\n",
    "    plt.show()\n",
    "    \n",
    "    count_ims += 1\n",
    "    \n",
    "    if count_ims == 3:\n",
    "        break\n",
    "\n",
    "# Fastai + distributed training\n",
    "from fastai              import *\n",
    "from fastai.torch_basics import *\n",
    "from fastai.basics       import *\n",
    "from fastai.distributed  import *\n",
    "from fastai.callback.all import SaveModelCallback, CSVLogger, ProgressCallback\n",
    "\n",
    "# clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from helpers.general import print_hardware_stats\n",
    "print_hardware_stats()\n",
    "\n",
    "# clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bs = 50\n",
    "\n",
    "train_dl = TfmdDL(train_datadict, after_item=train_transforms, after_batch=[], bs=bs)\n",
    "val_dl   = TfmdDL(valid_datadict,   after_item=valid_transforms,   after_batch=[], bs=bs)\n",
    "\n",
    "\n",
    "dls = DataLoaders(train_dl, val_dl)\n",
    "dls = dls.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c7100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.viz import *\n",
    "\n",
    "\n",
    "\n",
    "# UNET model\n",
    "model1 = UNet(\n",
    "                    dimensions=3,\n",
    "                    in_channels=1,\n",
    "                    out_channels=2,\n",
    "                    channels=(16, 32, 64, 128, 256),\n",
    "                    strides=(2, 2, 2, 2),\n",
    "                    num_res_units=2,\n",
    "                    dropout=0.0,\n",
    "                )\n",
    "\n",
    "save_model_dir = \"home/gologors/data/saved_models/transfer_learning_unet/fastai/1653509451_Wed_May_25_2022_hr_16_min_10\"\n",
    "\n",
    "# model.load_state_dict(torch.load(\n",
    "#     os.path.join(save_model_dir, \"best_metric_model.pth\")))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, val_data in enumerate(valid_loader):\n",
    "        val_inputs, val_labels = (\n",
    "            val_data[\"im\"].to(device),\n",
    "            val_data[\"lbl\"].to(device),\n",
    "        )\n",
    "        val_outputs = model(val_inputs)\n",
    "        # plot the slice [:, :, 80]\n",
    "        plt.figure(\"check\", (18, 6))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(f\"image {i}\")\n",
    "        plt.imshow(val_data[\"im\"][0, 0, :, :, center_crop_sz[2]//2], cmap=\"gray\")\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(f\"label {i}\")\n",
    "        plt.imshow(val_data[\"lbl\"][0, 0, :, :, center_crop_sz[2]//2])\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(f\"output {i}\")\n",
    "        plt.imshow(torch.argmax(\n",
    "            val_outputs, dim=1).detach().cpu()[0, :, :, 80])\n",
    "        plt.show()\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "viz_compare_outputs??\n",
    "\n",
    "val_outputs.shape\n",
    "\n",
    "val_inputs.shape\n",
    "\n",
    "val_inputs[0].cpu().numpy().shape\n",
    "\n",
    "viz_compare_outputs(val_inputs[0].cpu().squeeze(), val_labels[0].cpu().squeeze(), val_outputs[0].cpu().squeeze())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CustomEnv",
   "language": "python",
   "name": "custom_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
